{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import imageio\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from PIL import Image, ImageFile\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import albumentations\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import model_zoo\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install efficientnet_pytorch\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a methodology for dividing dataset into multiple folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stratification:\n",
    "    def __init__(self,path_dir,num_splits):\n",
    "        self.input_path = path_dir\n",
    "        self.n_splits = num_splits\n",
    "        self.df = pd.read_csv(os.path.join(input_path,\"train.csv\"))\n",
    "        \n",
    "    def create_split(self):\n",
    "        self.df['kfold'] = -1\n",
    "        #Shuffling the csv file => to get a new shuffled dataframe\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        #Target value\n",
    "        y=self.df.target.values\n",
    "        #Why stratified - because we want the ratio of +ve:-ve samples to be the same\n",
    "        kf = model_selection.StratifiedKFold(n_splits=self.n_splits)\n",
    "        \n",
    "        kfold_df_dict = {}\n",
    "        \n",
    "        for fold_, (train_idx, val_idx) in enumerate(kf.split(X=self.df,y=y)):\n",
    "            df_temp = pd.read_csv(os.path.join(input_path,\"train.csv\"))\n",
    "            df_temp['kfold'] = -1\n",
    "            df_temp['dataset_type'] = 'train'\n",
    "            df_temp.loc[:,'kfold']=fold_\n",
    "            df_temp.loc[val_idx,'dataset_type'] = 'val'\n",
    "            kfold_df_dict[fold_]=df_temp\n",
    "        \n",
    "        df_comb_fold = pd.concat(kfold_df_dict[k] for (k,v) in kfold_df_dict.items())\n",
    "        \n",
    "        return df_comb_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/kaggle/input/siim-isic-melanoma-classification\"\n",
    "num_splits = 2\n",
    "df_actual_train = pd.read_csv(\"/kaggle/input/siim-isic-melanoma-classification/train.csv\")\n",
    "df_kfold = stratification(input_path,num_splits).create_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now define a class for the model with the aim to:\n",
    "a.) defining the model with pre-trained weights\n",
    "b.) constructing a forward function that computes loss along with model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_efficientnetb7(torch.nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        \n",
    "        super(model_efficientnetb7,self).__init__()\n",
    "        \n",
    "        self.model = EfficientNet.from_pretrained('efficientnet-b7')\n",
    "        ## Changing the last layer\n",
    "        num_ftrs = self.model._fc.in_features\n",
    "        self.model._fc = nn.Linear(num_ftrs, 1)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def forward(self,image,targets):\n",
    "        # Arguments should match your dataloader arguments wrt dataset being passed\n",
    "        # in this case it is image, targets\n",
    "        \n",
    "        \n",
    "        out = self.model(image)\n",
    "        \n",
    "        loss = nn.BCEWithLogitsLoss()(\n",
    "            out, targets.reshape(-1,1).type_as(out)\n",
    "        )\n",
    "        # shape and datatype\n",
    "        return out,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationLoader:\n",
    "    def __init__(self, image_paths, targets, resize, augmentations):\n",
    "        self.image_paths = image_paths\n",
    "        self.targets = targets\n",
    "        self.resize = resize\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image = Image.open(self.image_paths[item])\n",
    "        targets = self.targets[item]\n",
    "        if self.resize is not None:\n",
    "            image = image.resize(\n",
    "                (self.resize[1], self.resize[0]), resample=Image.BILINEAR\n",
    "            )\n",
    "        image = np.array(image)\n",
    "        if self.augmentations is not None:\n",
    "            augmented = self.augmentations(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "        return {\n",
    "            \"image\": torch.tensor(image, dtype=torch.float),\n",
    "            \"targets\": torch.tensor(targets, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, mode=\"max\", delta=0.0001):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        if self.mode == \"min\":\n",
    "            self.val_score = np.Inf\n",
    "        else:\n",
    "            self.val_score = -np.Inf\n",
    "\n",
    "    def __call__(self, epoch_score, model, model_path):\n",
    "        if self.mode == \"min\":\n",
    "            score = -1.0 * epoch_score\n",
    "        else:\n",
    "            score = np.copy(epoch_score)\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(\n",
    "                \"EarlyStopping counter: {} out of {}\".format(\n",
    "                    self.counter, self.patience\n",
    "                )\n",
    "            )\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, epoch_score, model, model_path):\n",
    "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
    "            print(\n",
    "                \"Validation score improved ({} --> {}). Saving model!\".format(\n",
    "                    self.val_score, epoch_score\n",
    "                )\n",
    "            )\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        self.val_score = epoch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engine:\n",
    "    @staticmethod\n",
    "    def train(\n",
    "        data_loader,\n",
    "        model,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler=None,\n",
    "        accumulation_steps=1\n",
    "    ):\n",
    "        \n",
    "        losses = AverageMeter()\n",
    "        predictions = []\n",
    "        model.train()\n",
    "        if accumulation_steps > 1:\n",
    "            optimizer.zero_grad()\n",
    "        #tk0 = tqdm(data_loader, total=len(data_loader), disable=use_tpu)\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader),disable=False)\n",
    "        for b_idx, data in enumerate(tk0):\n",
    "            for key, value in data.items():\n",
    "                data[key] = value.to(device)\n",
    "            if accumulation_steps == 1 and b_idx == 0:\n",
    "                optimizer.zero_grad()\n",
    "            _, loss = model(**data)\n",
    "                   \n",
    "            with torch.set_grad_enabled(True):\n",
    "                loss.backward()\n",
    "                if (b_idx + 1) % accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    if scheduler is not None:\n",
    "                        scheduler.step()\n",
    "                    if b_idx > 0:\n",
    "                        optimizer.zero_grad()\n",
    "            losses.update(loss.item(), data_loader.batch_size)\n",
    "            tk0.set_postfix(loss=losses.avg)\n",
    "        return losses.avg\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(data_loader, model, device):\n",
    "        losses = AverageMeter()\n",
    "        final_predictions = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            #tk0 = tqdm(data_loader, total=len(data_loader), disable=use_tpu)\n",
    "            tk0 = tqdm(data_loader, total=len(data_loader), disable=False)\n",
    "            for b_idx, data in enumerate(tk0):\n",
    "                for key, value in data.items():\n",
    "                    data[key] = value.to(device)\n",
    "                predictions, loss = model(**data)\n",
    "                predictions = predictions.cpu()\n",
    "                losses.update(loss.item(), data_loader.batch_size)\n",
    "                final_predictions.append(predictions)\n",
    "                tk0.set_postfix(loss=losses.avg)\n",
    "        return final_predictions, losses.avg\n",
    "\n",
    "    @staticmethod\n",
    "    def predict(data_loader, model, device, use_tpu=False):\n",
    "        model.eval()\n",
    "        final_predictions = []\n",
    "        with torch.no_grad():\n",
    "            #tk0 = tqdm(data_loader, total=len(data_loader), disable=use_tpu)\n",
    "            tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "            for b_idx, data in enumerate(tk0):\n",
    "                for key, value in data.items():\n",
    "                    data[key] = value.to(device)\n",
    "                predictions, _ = model(**data)\n",
    "                predictions = predictions.cpu()\n",
    "                final_predictions.append(predictions)\n",
    "        return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fold):\n",
    "    print(f\"Starting Training for fold = {fold+1}\")\n",
    "    #Image size requirements per EfficientNet documentation\n",
    "    training_data_path = \"/kaggle/input/siic-isic-224x224-images/train\"\n",
    "    df = df_kfold[df_kfold['kfold']==fold]\n",
    "    device = 'cuda'\n",
    "    epochs = 50\n",
    "    train_bs = 4\n",
    "    val_bs = 4\n",
    "    \n",
    "    df_train = df.loc[df['dataset_type']=='train',list(df_actual_train.columns)]\n",
    "    df_val = df.loc[df['dataset_type']=='val',list(df_actual_train.columns)]\n",
    "    # Normalization needed as per EfficientNet documentation\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    # add any extra augmentations here\n",
    "    train_aug = albumentations.Compose(\n",
    "        [\n",
    "            albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True,p=1.0)\n",
    "        ]\n",
    "    )\n",
    "    val_aug = albumentations.Compose(\n",
    "        [\n",
    "            albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True,p=1.0)\n",
    "        ]\n",
    "    )\n",
    "    train_images_list = df_train.image_name.values.tolist()\n",
    "    train_images = [os.path.join(training_data_path,i + '.png') for i in train_images_list]\n",
    "    train_targets = df_train.target.values\n",
    "    \n",
    "    val_images_list = df_val.image_name.values.tolist()\n",
    "    val_images = [os.path.join(training_data_path,i + '.png') for i in val_images_list]\n",
    "    val_targets = df_val.target.values\n",
    "    \n",
    "    train_dataset = ClassificationLoader(\n",
    "        image_paths = train_images,\n",
    "        targets= train_targets,\n",
    "        resize = None,\n",
    "        augmentations = train_aug\n",
    "    )\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = train_bs,\n",
    "        shuffle = False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    val_dataset = ClassificationLoader(\n",
    "        image_paths = val_images,\n",
    "        targets= val_targets,\n",
    "        resize = None,\n",
    "        augmentations = val_aug\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size = val_bs,\n",
    "        shuffle = False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    #Earlier defined class for model\n",
    "    #model = Model_Inception_v3(pretrained='imagenet')\n",
    "    model = model_efficientnetb7()\n",
    "    model.to(device)\n",
    "    \n",
    "    #Specify an optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    #Specify an scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        patience=3,\n",
    "        mode='max'\n",
    "    )\n",
    "    # why mode='max' becauase we will be using the metric of AUC\n",
    "    \n",
    "    # we would also need early stopping\n",
    "    es = EarlyStopping(patience=5, mode='max')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        training_loss = Engine.train(\n",
    "            train_loader,\n",
    "            model,\n",
    "            optimizer,\n",
    "            device\n",
    "        )\n",
    "        predictions, val_loss = Engine.evaluate(\n",
    "            val_loader,\n",
    "            model,\n",
    "            device\n",
    "        )\n",
    "        \n",
    "        predictions = np.vstack((predictions)).ravel()\n",
    "        # Ravel it because we have only one value\n",
    "        auc = metrics.roc_auc_score(val_targets, predictions)\n",
    "        # thats why val_loader shuffle was kept false\n",
    "        \n",
    "        scheduler.step(auc)\n",
    "        print(f\"epoch={epoch},auc={auc}\")\n",
    "        # Save it with .bin extension\n",
    "        model_path = f'efficient_model_fold{fold}.bin'\n",
    "        es(auc, model, model_path)\n",
    "        if es.early_stop:\n",
    "            print(\"Early Stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(fold):\n",
    "    print(f\"Generating Predictions for saved model, fold = {fold+1}\")\n",
    "    test_data_path = \"/kaggle/input/siic-isic-224x224-images/test\"\n",
    "    df_test = pd.read_csv(\"/kaggle/input/siim-isic-melanoma-classification/test.csv\")\n",
    "    df_test.loc[:,'target'] = 0\n",
    "    \n",
    "    #model_path = \"f'/kaggle/working/model_fold{fold}'\"\n",
    "    #model_path = '/kaggle/working/model_fold0_epoch0.bin'\n",
    "    model_path = f'/kaggle/working/efficient_model_fold{fold}.bin'\n",
    "    \n",
    "    device = 'cuda'\n",
    "    \n",
    "    test_bs = 16\n",
    "    \n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    \n",
    "    test_aug = albumentations.Compose(\n",
    "        [\n",
    "            albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True,p=1.0)\n",
    "        ]\n",
    "    )\n",
    "    test_images_list = df_test.image_name.values.tolist()\n",
    "    test_images = [os.path.join(test_data_path,i + '.png') for i in test_images_list]\n",
    "    test_targets = df_test.target.values\n",
    "    \n",
    "    test_dataset = ClassificationLoader(\n",
    "        image_paths = test_images,\n",
    "        targets= test_targets,\n",
    "        resize = None,\n",
    "        augmentations = test_aug\n",
    "    )\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size = test_bs,\n",
    "        shuffle = False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    #Earlier defined class for model\n",
    "    model = model_efficientnetb7()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    \n",
    "    predictions_op = Engine.predict(\n",
    "        test_loader,\n",
    "        model,\n",
    "        device\n",
    "    )\n",
    "    return np.vstack((predictions_op)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "for fold_ in range(num_splits):\n",
    "    torch.cuda.empty_cache()\n",
    "    train(fold=fold_)\n",
    "    list_fold_pred.append(predict(fold_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Predictions\n",
    "list_fold_pred = []\n",
    "for fold_ in range(num_splits):    \n",
    "    list_fold_pred.append(predict(fold_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.mean(np.vstack(list(list_fold_pred[i] for i in range(1))),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(\"../input/siim-isic-melanoma-classification/sample_submission.csv\")\n",
    "sample.loc[:, \"target\"] = pred\n",
    "sample.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
