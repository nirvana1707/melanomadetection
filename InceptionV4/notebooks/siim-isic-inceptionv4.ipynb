{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import imageio\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from PIL import Image, ImageFile\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import albumentations\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import model_zoo\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes,\n",
    "                              kernel_size=kernel_size, stride=stride,\n",
    "                              padding=padding, bias=False) # verify bias false\n",
    "        self.bn = nn.BatchNorm2d(out_planes,\n",
    "                                 eps=0.001, # value found in tensorflow\n",
    "                                 momentum=0.1, # default pytorch value\n",
    "                                 affine=True)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mixed_3a(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Mixed_3a, self).__init__()\n",
    "        self.maxpool = nn.MaxPool2d(3, stride=2)\n",
    "        self.conv = BasicConv2d(64, 96, kernel_size=3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.maxpool(x)\n",
    "        x1 = self.conv(x)\n",
    "        out = torch.cat((x0, x1), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mixed_4a(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Mixed_4a, self).__init__()\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "            BasicConv2d(160, 64, kernel_size=1, stride=1),\n",
    "            BasicConv2d(64, 96, kernel_size=3, stride=1)\n",
    "        )\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(160, 64, kernel_size=1, stride=1),\n",
    "            BasicConv2d(64, 64, kernel_size=(1,7), stride=1, padding=(0,3)),\n",
    "            BasicConv2d(64, 64, kernel_size=(7,1), stride=1, padding=(3,0)),\n",
    "            BasicConv2d(64, 96, kernel_size=(3,3), stride=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        out = torch.cat((x0, x1), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Mixed_5a(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Mixed_5a, self).__init__()\n",
    "        self.conv = BasicConv2d(192, 192, kernel_size=3, stride=2)\n",
    "        self.maxpool = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.conv(x)\n",
    "        x1 = self.maxpool(x)\n",
    "        out = torch.cat((x0, x1), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Inception_A(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Inception_A, self).__init__()\n",
    "        self.branch0 = BasicConv2d(384, 96, kernel_size=1, stride=1)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(384, 64, kernel_size=1, stride=1),\n",
    "            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(384, 64, kernel_size=1, stride=1),\n",
    "            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n",
    "            BasicConv2d(384, 96, kernel_size=1, stride=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        out = torch.cat((x0, x1, x2, x3), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Reduction_A(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Reduction_A, self).__init__()\n",
    "        self.branch0 = BasicConv2d(384, 384, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(384, 192, kernel_size=1, stride=1),\n",
    "            BasicConv2d(192, 224, kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(224, 256, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        out = torch.cat((x0, x1, x2), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Inception_B(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Inception_B, self).__init__()\n",
    "        self.branch0 = BasicConv2d(1024, 384, kernel_size=1, stride=1)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n",
    "            BasicConv2d(192, 224, kernel_size=(1,7), stride=1, padding=(0,3)),\n",
    "            BasicConv2d(224, 256, kernel_size=(7,1), stride=1, padding=(3,0))\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n",
    "            BasicConv2d(192, 192, kernel_size=(7,1), stride=1, padding=(3,0)),\n",
    "            BasicConv2d(192, 224, kernel_size=(1,7), stride=1, padding=(0,3)),\n",
    "            BasicConv2d(224, 224, kernel_size=(7,1), stride=1, padding=(3,0)),\n",
    "            BasicConv2d(224, 256, kernel_size=(1,7), stride=1, padding=(0,3))\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n",
    "            BasicConv2d(1024, 128, kernel_size=1, stride=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        out = torch.cat((x0, x1, x2, x3), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Reduction_B(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Reduction_B, self).__init__()\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n",
    "            BasicConv2d(192, 192, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(1024, 256, kernel_size=1, stride=1),\n",
    "            BasicConv2d(256, 256, kernel_size=(1,7), stride=1, padding=(0,3)),\n",
    "            BasicConv2d(256, 320, kernel_size=(7,1), stride=1, padding=(3,0)),\n",
    "            BasicConv2d(320, 320, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        out = torch.cat((x0, x1, x2), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Inception_C(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Inception_C, self).__init__()\n",
    "\n",
    "        self.branch0 = BasicConv2d(1536, 256, kernel_size=1, stride=1)\n",
    "\n",
    "        self.branch1_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)\n",
    "        self.branch1_1a = BasicConv2d(384, 256, kernel_size=(1,3), stride=1, padding=(0,1))\n",
    "        self.branch1_1b = BasicConv2d(384, 256, kernel_size=(3,1), stride=1, padding=(1,0))\n",
    "\n",
    "        self.branch2_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)\n",
    "        self.branch2_1 = BasicConv2d(384, 448, kernel_size=(3,1), stride=1, padding=(1,0))\n",
    "        self.branch2_2 = BasicConv2d(448, 512, kernel_size=(1,3), stride=1, padding=(0,1))\n",
    "        self.branch2_3a = BasicConv2d(512, 256, kernel_size=(1,3), stride=1, padding=(0,1))\n",
    "        self.branch2_3b = BasicConv2d(512, 256, kernel_size=(3,1), stride=1, padding=(1,0))\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n",
    "            BasicConv2d(1536, 256, kernel_size=1, stride=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "\n",
    "        x1_0 = self.branch1_0(x)\n",
    "        x1_1a = self.branch1_1a(x1_0)\n",
    "        x1_1b = self.branch1_1b(x1_0)\n",
    "        x1 = torch.cat((x1_1a, x1_1b), 1)\n",
    "\n",
    "        x2_0 = self.branch2_0(x)\n",
    "        x2_1 = self.branch2_1(x2_0)\n",
    "        x2_2 = self.branch2_2(x2_1)\n",
    "        x2_3a = self.branch2_3a(x2_2)\n",
    "        x2_3b = self.branch2_3b(x2_2)\n",
    "        x2 = torch.cat((x2_3a, x2_3b), 1)\n",
    "\n",
    "        x3 = self.branch3(x)\n",
    "\n",
    "        out = torch.cat((x0, x1, x2, x3), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class InceptionV4(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1001):\n",
    "        super(InceptionV4, self).__init__()\n",
    "        # Special attributs\n",
    "        self.input_space = None\n",
    "        self.input_size = (299, 299, 3)\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        # Modules\n",
    "        self.features = nn.Sequential(\n",
    "            BasicConv2d(3, 32, kernel_size=3, stride=2),\n",
    "            BasicConv2d(32, 32, kernel_size=3, stride=1),\n",
    "            BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            Mixed_3a(),\n",
    "            Mixed_4a(),\n",
    "            Mixed_5a(),\n",
    "            Inception_A(),\n",
    "            Inception_A(),\n",
    "            Inception_A(),\n",
    "            Inception_A(),\n",
    "            Reduction_A(), # Mixed_6a\n",
    "            Inception_B(),\n",
    "            Inception_B(),\n",
    "            Inception_B(),\n",
    "            Inception_B(),\n",
    "            Inception_B(),\n",
    "            Inception_B(),\n",
    "            Inception_B(),\n",
    "            Reduction_B(), # Mixed_7a\n",
    "            Inception_C(),\n",
    "            Inception_C(),\n",
    "            Inception_C()\n",
    "        )\n",
    "        self.last_linear = nn.Linear(1536, num_classes)\n",
    "\n",
    "    def logits(self, features):\n",
    "        #Allows image of any size to be processed\n",
    "        adaptiveAvgPoolWidth = features.shape[2]\n",
    "        x = F.avg_pool2d(features, kernel_size=adaptiveAvgPoolWidth)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.features(input)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def inceptionv4(num_classes=1000, pretrained='imagenet'):\n",
    "    if pretrained:\n",
    "        settings = pretrained_settings['inceptionv4'][pretrained]\n",
    "        assert num_classes == settings['num_classes'], \\\n",
    "            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n",
    "\n",
    "        # both 'imagenet'&'imagenet+background' are loaded from same parameters\n",
    "        model = InceptionV4(num_classes=1001)\n",
    "        #model.load_state_dict(model_zoo.load_url(settings['url']))\n",
    "        path_weight = \"/kaggle/input/pytorch-pretrained-models/inceptionv4-8e4777a0.pth\"\n",
    "        model.load_state_dict(torch.load(path_weight))\n",
    "\n",
    "        if pretrained == 'imagenet':\n",
    "            new_last_linear = nn.Linear(1536, 1000)\n",
    "            new_last_linear.weight.data = model.last_linear.weight.data[1:]\n",
    "            new_last_linear.bias.data = model.last_linear.bias.data[1:]\n",
    "            model.last_linear = new_last_linear\n",
    "\n",
    "        model.input_space = settings['input_space']\n",
    "        model.input_size = settings['input_size']\n",
    "        model.input_range = settings['input_range']\n",
    "        model.mean = settings['mean']\n",
    "        model.std = settings['std']\n",
    "    else:\n",
    "        model = InceptionV4(num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stratification:\n",
    "    def __init__(self,path_dir,num_splits):\n",
    "        self.input_path = path_dir\n",
    "        self.n_splits = num_splits\n",
    "        self.df = pd.read_csv(os.path.join(input_path,\"train.csv\"))\n",
    "        \n",
    "    def create_split(self):\n",
    "        self.df['kfold'] = -1\n",
    "        #Shuffling the csv file => to get a new shuffled dataframe\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        #Target value\n",
    "        y=self.df.target.values\n",
    "        #Why stratified - because we want the ratio of +ve:-ve samples to be the same\n",
    "        kf = model_selection.StratifiedKFold(n_splits=self.n_splits)\n",
    "        \n",
    "        kfold_df_dict = {}\n",
    "        \n",
    "        for fold_, (train_idx, val_idx) in enumerate(kf.split(X=self.df,y=y)):\n",
    "            df_temp = pd.read_csv(os.path.join(input_path,\"train.csv\"))\n",
    "            df_temp['kfold'] = -1\n",
    "            df_temp['dataset_type'] = 'train'\n",
    "            df_temp.loc[:,'kfold']=fold_\n",
    "            df_temp.loc[val_idx,'dataset_type'] = 'val'\n",
    "            kfold_df_dict[fold_]=df_temp\n",
    "        \n",
    "        df_comb_fold = pd.concat(kfold_df_dict[k] for (k,v) in kfold_df_dict.items())\n",
    "        \n",
    "        return df_comb_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/kaggle/input/siim-isic-melanoma-classification\"\n",
    "num_splits = 10\n",
    "df_actual_train = pd.read_csv(\"/kaggle/input/siim-isic-melanoma-classification/train.csv\")\n",
    "df_kfold = stratification(input_path,num_splits).create_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_inceptionv4(torch.nn.Module):\n",
    "    def __init__(self, pretrained='imagenet'):\n",
    "        #The super() function is used to give access to methods and properties of a parent or sibling class.\n",
    "        #The super() function returns an object that represents the parent class.\n",
    "        #super(Model_ResNext_Pytorch,self).__init__()\n",
    "        super(model_inceptionv4,self).__init__()\n",
    "        #self.model = pretrainedmodels.__dict__[\"se_resnext50_32x4d\"]\n",
    "        #self.model = torch.hub.load('pytorch/vision:v0.6.0', 'resnext50_32x4d', pretrained=True)\n",
    "        #self.model = torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', pretrained=pretrained)\n",
    "        self.model = inceptionv4(pretrained=pretrained)\n",
    "        #self.out = nn.Linear(2048,1)\n",
    "        self.out = nn.Linear(1536,1)\n",
    "    \n",
    "    def forward(self,image,targets):\n",
    "        # Arguments should match your dataloader arguments wrt dataset being passed\n",
    "        # in this case it is image, targets\n",
    "        \n",
    "        bs, _, _, _ = image.shape\n",
    "        x = self.model.features(image)\n",
    "        x = F.adaptive_avg_pool2d(x,1)\n",
    "        x = x.reshape(bs,-1)\n",
    "        out = self.out(x)\n",
    "        loss = nn.BCEWithLogitsLoss()(\n",
    "            out, targets.reshape(-1,1).type_as(out)\n",
    "        )\n",
    "        # shape and datatype\n",
    "        return out,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationLoader:\n",
    "    def __init__(self, image_paths, targets, resize, augmentations):\n",
    "        self.image_paths = image_paths\n",
    "        self.targets = targets\n",
    "        self.resize = resize\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image = Image.open(self.image_paths[item])\n",
    "        targets = self.targets[item]\n",
    "        if self.resize is not None:\n",
    "            image = image.resize(\n",
    "                (self.resize[1], self.resize[0]), resample=Image.BILINEAR\n",
    "            )\n",
    "        image = np.array(image)\n",
    "        if self.augmentations is not None:\n",
    "            augmented = self.augmentations(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "        return {\n",
    "            \"image\": torch.tensor(image, dtype=torch.float),\n",
    "            \"targets\": torch.tensor(targets, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, mode=\"max\", delta=0.0001):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        if self.mode == \"min\":\n",
    "            self.val_score = np.Inf\n",
    "        else:\n",
    "            self.val_score = -np.Inf\n",
    "\n",
    "    def __call__(self, epoch_score, model, model_path):\n",
    "        if self.mode == \"min\":\n",
    "            score = -1.0 * epoch_score\n",
    "        else:\n",
    "            score = np.copy(epoch_score)\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(\n",
    "                \"EarlyStopping counter: {} out of {}\".format(\n",
    "                    self.counter, self.patience\n",
    "                )\n",
    "            )\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, epoch_score, model, model_path):\n",
    "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
    "            print(\n",
    "                \"Validation score improved ({} --> {}). Saving model!\".format(\n",
    "                    self.val_score, epoch_score\n",
    "                )\n",
    "            )\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        self.val_score = epoch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engine:\n",
    "    @staticmethod\n",
    "    def train(\n",
    "        data_loader,\n",
    "        model,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler=None,\n",
    "        accumulation_steps=1\n",
    "    ):\n",
    "        \n",
    "        losses = AverageMeter()\n",
    "        predictions = []\n",
    "        model.train()\n",
    "        if accumulation_steps > 1:\n",
    "            optimizer.zero_grad()\n",
    "        #tk0 = tqdm(data_loader, total=len(data_loader), disable=use_tpu)\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader),disable=False)\n",
    "        for b_idx, data in enumerate(tk0):\n",
    "            for key, value in data.items():\n",
    "                data[key] = value.to(device)\n",
    "            if accumulation_steps == 1 and b_idx == 0:\n",
    "                optimizer.zero_grad()\n",
    "            _, loss = model(**data)\n",
    "                   \n",
    "            with torch.set_grad_enabled(True):\n",
    "                loss.backward()\n",
    "                if (b_idx + 1) % accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    if scheduler is not None:\n",
    "                        scheduler.step()\n",
    "                    if b_idx > 0:\n",
    "                        optimizer.zero_grad()\n",
    "            losses.update(loss.item(), data_loader.batch_size)\n",
    "            tk0.set_postfix(loss=losses.avg)\n",
    "        return losses.avg\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(data_loader, model, device):\n",
    "        losses = AverageMeter()\n",
    "        final_predictions = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            #tk0 = tqdm(data_loader, total=len(data_loader), disable=use_tpu)\n",
    "            tk0 = tqdm(data_loader, total=len(data_loader), disable=False)\n",
    "            for b_idx, data in enumerate(tk0):\n",
    "                for key, value in data.items():\n",
    "                    data[key] = value.to(device)\n",
    "                predictions, loss = model(**data)\n",
    "                predictions = predictions.cpu()\n",
    "                losses.update(loss.item(), data_loader.batch_size)\n",
    "                final_predictions.append(predictions)\n",
    "                tk0.set_postfix(loss=losses.avg)\n",
    "        return final_predictions, losses.avg\n",
    "\n",
    "    @staticmethod\n",
    "    def predict(data_loader, model, device, use_tpu=False):\n",
    "        model.eval()\n",
    "        final_predictions = []\n",
    "        with torch.no_grad():\n",
    "            #tk0 = tqdm(data_loader, total=len(data_loader), disable=use_tpu)\n",
    "            tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "            for b_idx, data in enumerate(tk0):\n",
    "                for key, value in data.items():\n",
    "                    data[key] = value.to(device)\n",
    "                predictions, _ = model(**data)\n",
    "                predictions = predictions.cpu()\n",
    "                final_predictions.append(predictions)\n",
    "        return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(fold):\n",
    "    training_data_path = \"/kaggle/input/siic-isic-224x224-images/train\"\n",
    "    df = df_kfold[df_kfold['kfold']==fold]\n",
    "    device = 'cuda'\n",
    "    epochs = 50\n",
    "    train_bs = 32\n",
    "    val_bs = 16\n",
    "    \n",
    "    df_train = df.loc[df['dataset_type']=='train',list(df_actual_train.columns)]\n",
    "    df_val = df.loc[df['dataset_type']=='val',list(df_actual_train.columns)]\n",
    "    \n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    train_aug = albumentations.Compose(\n",
    "        [\n",
    "            albumentations.Resize(299, 299, p=1.0),\n",
    "            albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True,p=1.0)\n",
    "        ]\n",
    "    )\n",
    "    val_aug = albumentations.Compose(\n",
    "        [\n",
    "            albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True,p=1.0)\n",
    "        ]\n",
    "    )\n",
    "    train_images_list = df_train.image_name.values.tolist()\n",
    "    train_images = [os.path.join(training_data_path,i + '.png') for i in train_images_list]\n",
    "    train_targets = df_train.target.values\n",
    "    \n",
    "    val_images_list = df_val.image_name.values.tolist()\n",
    "    val_images = [os.path.join(training_data_path,i + '.png') for i in val_images_list]\n",
    "    val_targets = df_val.target.values\n",
    "    \n",
    "    train_dataset = ClassificationLoader(\n",
    "        image_paths = train_images,\n",
    "        targets= train_targets,\n",
    "        resize = None,\n",
    "        augmentations = train_aug\n",
    "    )\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = train_bs,\n",
    "        shuffle = False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    val_dataset = ClassificationLoader(\n",
    "        image_paths = val_images,\n",
    "        targets= val_targets,\n",
    "        resize = None,\n",
    "        augmentations = val_aug\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size = val_bs,\n",
    "        shuffle = False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    #Earlier defined class for model\n",
    "    model = model_inceptionv4(pretrained='imagenet')\n",
    "    model.to(device)\n",
    "    \n",
    "    #Specify an optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    #Specify an scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        patience=3,\n",
    "        mode='max'\n",
    "    )\n",
    "    # why mode='max' becauase we will be using the metric of AUC\n",
    "    \n",
    "    # we would also need early stopping\n",
    "    es = EarlyStopping(patience=5, mode='max')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        training_loss = Engine.train(\n",
    "            train_loader,\n",
    "            model,\n",
    "            optimizer,\n",
    "            device\n",
    "        )\n",
    "        predictions, val_loss = Engine.evaluate(\n",
    "            val_loader,\n",
    "            model,\n",
    "            device\n",
    "        )\n",
    "        \n",
    "        predictions = np.vstack((predictions)).ravel()\n",
    "        # Ravel it because we have only one value\n",
    "        auc = metrics.roc_auc_score(val_targets, predictions)\n",
    "        # thats why val_loader shuffle was kept false\n",
    "        \n",
    "        scheduler.step(auc)\n",
    "        print(f\"epoch={epoch},auc={auc}\")\n",
    "        # Save it with .bin extension\n",
    "        model_path = f'model_fold{fold}_epoch{epoch}.bin'\n",
    "        es(auc, model, model_path)\n",
    "        if es.early_stop:\n",
    "            print(\"Early Stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_fold_pred = []\n",
    "for fold_ in range(num_splits):\n",
    "    torch.cuda.empty_cache()\n",
    "    train(fold=fold_)\n",
    "    list_fold_pred.append(predict(fold_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
